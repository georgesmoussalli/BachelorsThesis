{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import cv2\n",
    "from pdf2image import convert_from_path\n",
    "from IPython.display import display\n",
    "import pytesseract\n",
    "import unicodedata\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import natsort\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = '/opt/homebrew/bin/tesseract'\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)      # Show all rows\n",
    "pd.set_option(\"display.max_columns\", None)   # Show all columns\n",
    "pd.set_option(\"display.width\", 0)            # Auto-detect width\n",
    "pd.set_option(\"display.max_colwidth\", None)  # Don't truncate column content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the year from the filename after 'Thèses_'.\n",
    "def extract_year_from_filename(filename):\n",
    "    # Regex pattern to match both formats\n",
    "    match = re.search(r'Theses_(\\d{4})(?:_(\\d{4}))?_(\\d+)_(\\d+|blank)(?:\\.pdf)?$', filename)\n",
    "    \n",
    "    # If no match is found, return None or handle as needed\n",
    "    if not match:\n",
    "        print(f\" No match for filename: {filename}\")\n",
    "        return None\n",
    "\n",
    "    # Extract the year and check if there's a second year\n",
    "    year = match.group(1)\n",
    "    second_year = match.group(2)  # This will be None if there's no second year\n",
    "    start = match.group(3)\n",
    "    end = match.group(4)\n",
    "\n",
    "    # If a second year exists, concatenate it with the first year\n",
    "    if second_year:\n",
    "        year = f\"{year}_{second_year}\"\n",
    "\n",
    "    return (year, int(start), end)\n",
    "\n",
    "#Checks if a page is blank \n",
    "def is_blank(image, threshold=0.99):\n",
    "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
    "    height = gray.shape[0]\n",
    "    \n",
    "    # Crops the 10% at the bottom of the page so that the number of the page doesn't affect our analysis\n",
    "    cropped_gray = gray[:int(0.9 * height), :]\n",
    "    \n",
    "    _, thresh = cv2.threshold(cropped_gray, 240, 255, cv2.THRESH_BINARY)\n",
    "    white_ratio = np.sum(thresh == 255) / thresh.size\n",
    "    \n",
    "    return white_ratio > threshold  # Retourne True si la page est blanche\n",
    "\n",
    "# Converts a PDF file into images, saving each page as a PNG.\n",
    "# # Images are stored in the specified output folder.\n",
    "def convert_pdf_to_images(pdf_path, output_folder, start_page, end_page):\n",
    "    images = convert_from_path(pdf_path)\n",
    "    if end_page =='blank' :\n",
    "        images = images[start_page:] \n",
    "    else : \n",
    "        end_page = int(end_page)\n",
    "        images = images[start_page:end_page]  \n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        image_path = os.path.join(output_folder, f\"page_{i+1}.png\")\n",
    "        image.save(image_path, \"PNG\")\n",
    "        print(f\"Page {i+1} saved: {image_path}\")\n",
    "\n",
    "        if is_blank(image):\n",
    "            print(f\"Page {i+1} est blanche. Arrêt du traitement.\")\n",
    "            break\n",
    "\n",
    "# Processes all PDFs in the input directory.\n",
    "# Extracts the year from the filename and creates a subfolder for each year.\n",
    "# Converts each PDF into images and stores them in the corresponding subfolder.\n",
    "def process_all_pdfs(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure the parent directory exists\n",
    "    \n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(input_dir, filename)  # Full path to PDF\n",
    "            year, start_page, end_page = extract_year_from_filename(filename)\n",
    "            print(f\"Extracted year: {year}\")  # Debugging\n",
    "            if not year or year == \"Erreur\":  # Skip if invalid year extracted\n",
    "                print(f\" Invalid year extracted: {year} for {filename}\")\n",
    "                continue\n",
    "\n",
    "            output_folder = os.path.join(output_dir, year)\n",
    "            print(f\"Creating subfolder: {output_folder}\")  # Debugging\n",
    "            os.makedirs(output_folder, exist_ok=True)  # Ensure the subfolder is created\n",
    "            \n",
    "            convert_pdf_to_images(pdf_path, output_folder,start_page,end_page)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "\n",
    "# Define the input and output paths relative to the current directory\n",
    "input_dir = os.path.join(current_dir, '..', 'data', 'pdf_Tables_theses_Paris_1870_1939')  # Relative path to 'lib/data/pdf_Tables_theses_Paris_1870_1939'\n",
    "output_dir = os.path.join(current_dir, '..', 'data', 'pdfs_en_images_png')  # Output path within 'lib/data/pdfs_en_images_png'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok = True)\n",
    "print(f\"Output directory: {output_dir}\")  # Debugging line to check the output path\n",
    "\n",
    "process_all_pdfs(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIN DE L'EXTRACTION DE TEXTE;\n",
    "DEBUT DE L'EXTRACTIION ET DU TRAITEMENT DU TEXTE.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_informations(text):\n",
    "\n",
    "    pattern = re.compile(r\"([A-ZÉÀÈÙÂÊÎÔÛÄËÏÖÜÇ]{2,}) \\(([^)]+)\\)\\. (.+)?\")\n",
    "\n",
    "    # Initialisation des variables\n",
    "    data = []\n",
    "    current_nom = None\n",
    "    current_prenom = None\n",
    "    current_sujet = \"\"                      \n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()  \n",
    "        \n",
    "        match = pattern.match(line)  \n",
    "\n",
    "        if match:\n",
    "            if current_nom is not None:\n",
    "                data.append([current_nom, current_prenom, current_sujet.strip()])\n",
    "\n",
    "            current_nom = match.group(1)\n",
    "            current_prenom = match.group(2)\n",
    "            current_sujet = match.group(3) if match.group(3) else \"\"\n",
    "        \n",
    "        else:\n",
    "            if current_nom is not None:\n",
    "                current_sujet += \" \" + line  \n",
    "\n",
    "    if current_nom is not None:\n",
    "        data.append([current_nom, current_prenom, current_sujet.strip()])\n",
    "\n",
    "    df_cleaned = pd.DataFrame(data, columns=[\"Nom\", \"Prénom\", \"Sujet\"])\n",
    "\n",
    "    return df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescue_missing_entries(df):\n",
    "    \"\"\"\n",
    "    Looks for names accidentally merged into the 'Sujet' field,\n",
    "    and splits them out into new rows.\n",
    "    \"\"\"\n",
    "    rescue_pattern = re.compile(\n",
    "        r\"\\b([A-ZÉÀÈÙÂÊÎÔÛÄËÏÖÜÇ]{2,})[ ,]*([A-ZÉÀÈÙÂÊÎÔÛÄËÏÖÜÇa-zéàèùâêîôûäëïöüç\\-]+)\\)?[ .,-]\"\n",
    "    )\n",
    "\n",
    "    rescued_rows = []\n",
    "\n",
    "    for idx, sujet in df[\"Sujet\"].items():\n",
    "        matches = list(rescue_pattern.finditer(sujet))\n",
    "\n",
    "        if matches:\n",
    "            match = matches[0]\n",
    "            start = match.start()\n",
    "            rescued_text = sujet[start:]\n",
    "            original_subject = sujet[:start].strip()\n",
    "\n",
    "            # Update current row's Sujet\n",
    "            df.at[idx, \"Sujet\"] = original_subject\n",
    "\n",
    "            # Create rescued row\n",
    "            nom = match.group(1)\n",
    "            prenom = match.group(2)\n",
    "            sujet_rescue = rescued_text[len(match.group(0)):].strip()\n",
    "\n",
    "            rescued_rows.append({\n",
    "                \"Nom\": nom,\n",
    "                \"Prénom\": prenom,\n",
    "                \"Sujet\": sujet_rescue\n",
    "            })\n",
    "\n",
    "    if rescued_rows:\n",
    "        df_rescue = pd.DataFrame(rescued_rows)\n",
    "        df = pd.concat([df, df_rescue], ignore_index=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image, first) : \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    gray = clahe.apply(gray)\n",
    "\n",
    "    (h, w) = gray.shape\n",
    "    \n",
    "    if first == True : \n",
    "        gray = gray[580:, :]  # Start from row 580 \n",
    "\n",
    "    left_part = gray[:, :w//2 - 5]   \n",
    "    right_part = gray[:,  5 + w//2:]\n",
    "\n",
    "\n",
    "    left_text = pytesseract.image_to_string(left_part, lang = 'fra')\n",
    "    right_text = pytesseract.image_to_string(right_part, lang = 'fra')\n",
    "\n",
    "    full_text = left_text + \" \" + right_text\n",
    "\n",
    "    return full_text \n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "input_dir = os.path.join(current_dir, '..', 'data', 'pdfs_en_images_png','1870','page_3.png')  # Relative path to 'lib/data/pdfs_en_images_png'\n",
    "image = cv2.imread(input_dir)\n",
    "\n",
    "process_image(image, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(text):\n",
    "    if isinstance(text, str):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', text)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "    return text\n",
    "\n",
    "def cleanup_up_subject_column(df) :\n",
    "    df['Nom'] = df['Nom'].astype(str)\n",
    "    df['Prénom'] = df['Prénom'].astype(str)\n",
    "    df['Sujet'] = df['Sujet'].astype(str)\n",
    "    df['Sujet'] = df['Sujet'].str.replace('- ', '', regex=False)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r',?\\s*\\d+\\s*', ' ', regex=True)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r'\\s+', ' ', regex=True).str.strip() \n",
    "    df['Sujet'] = df['Sujet'].str.replace(r\"([bcdfghjklmnpqrstvwxyz]) (?=[aeiouy])\", r\"\\1\", regex=True)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r\"[\\\"'#%&*\\[\\]{}<>|\\\\/^¤§°@=+\\~`]\", \"\", regex=True)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r\"[•●▪■♦◊¤§°@©®™‰¨«»„”“†‡‚‘’]\", \" \", regex=True)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r\"\\b[a-zA-Z]\\b\", \"\", regex=True)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r\"\\b(TOM|TON|H|U|UVIL|co|tu|ot|mw|vf|En|El|N)\\b\", \"\", regex=True, flags=re.IGNORECASE)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r\"\\s*[\\.,;:!?]\\s*\", \". \", regex=True)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    df['Sujet'] = df['Sujet'].apply(remove_accents)\n",
    "    df['Prénom'] = df['Prénom'].apply(remove_accents)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(input_dir):\n",
    "    text = \"\"  \n",
    "    df = pd.DataFrame()\n",
    "  \n",
    "    \n",
    "    for image_file in natsort.natsorted(os.listdir(input_dir)):\n",
    "        image_path = os.path.join(input_dir, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        print(image_path)\n",
    "        if image_file.lower().endswith('.png'): \n",
    "            if  image_file == \"page_1.png\" : \n",
    "                first = True \n",
    "            else : first = False\n",
    "            print(f\"Processing {image_path}...\")\n",
    "            text = text + process_image(image, first)\n",
    "    df = extract_informations(text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_folders(input_dir) : \n",
    "    df= pd.DataFrame(columns=[\"Nom\", \"Prénom\", \"Sujet\"])\n",
    "\n",
    "    for year_folder in natsort.natsorted(os.listdir(input_dir)):\n",
    "        year_path = os.path.join(input_dir, year_folder) \n",
    "        \n",
    "        if os.path.isdir(year_path):  # Check if it's a directory\n",
    "            df = pd.concat((df, process_folder(year_path)), ignore_index= True)\n",
    "            rescue_missing_entries(df)\n",
    "            cleanup_up_subject_column(df)\n",
    "            print(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "input_dir = os.path.join(current_dir, '..', 'data', 'pdfs_en_images_png')  # Relative path to 'lib/data/pdfs_en_images_png'\n",
    "\n",
    "process_all_folders(input_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
