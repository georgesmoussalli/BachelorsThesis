{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import cv2\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd\n",
    "import natsort\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = '/opt/homebrew/bin/tesseract'\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)      # Show all rows\n",
    "pd.set_option(\"display.max_columns\", None)   # Show all columns\n",
    "pd.set_option(\"display.width\", 0)            # Auto-detect width\n",
    "pd.set_option(\"display.max_colwidth\", None)  # Don't truncate column content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the year from the filename after 'Thèses_'.\n",
    "def extract_year_from_filename(filename):\n",
    "    # Regex pattern to match both formats\n",
    "    match = re.search(r'Theses_(\\d{4})(?:_(\\d{4}))?_(\\d+)_(\\d+|blank)(?:\\.pdf)?$', filename)\n",
    "    \n",
    "    # If no match is found, return None or handle as needed\n",
    "    if not match:\n",
    "        return None\n",
    "\n",
    "    # Extract the year and check if there's a second year\n",
    "    year = match.group(1)\n",
    "    second_year = match.group(2)  # This will be None if there's no second year\n",
    "    start = match.group(3)\n",
    "    end = match.group(4)\n",
    "\n",
    "    # If a second year exists, concatenate it with the first year\n",
    "    if second_year:\n",
    "        year = f\"{year}_{second_year}\"\n",
    "\n",
    "    return (year, int(start), end)\n",
    "\n",
    "#Checks if a page is blank \n",
    "def is_blank(image, threshold=0.99):\n",
    "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
    "    height = gray.shape[0]\n",
    "    \n",
    "    # Crops the 10% at the bottom of the page so that the number of the page doesn't affect our analysis\n",
    "    cropped_gray = gray[:int(0.9 * height), :]\n",
    "    \n",
    "    _, thresh = cv2.threshold(cropped_gray, 240, 255, cv2.THRESH_BINARY)\n",
    "    white_ratio = np.sum(thresh == 255) / thresh.size\n",
    "    \n",
    "    return white_ratio > threshold  # Retourne True si la page est blanche\n",
    "\n",
    "# Converts a PDF file into images, saving each page as a PNG.\n",
    "# # Images are stored in the specified output folder.\n",
    "def convert_pdf_to_images(pdf_path, output_folder, start_page, end_page):\n",
    "    images = convert_from_path(pdf_path)\n",
    "    if end_page =='blank' :\n",
    "        images = images[start_page:] \n",
    "    else : \n",
    "        end_page = int(end_page)\n",
    "        images = images[start_page:end_page]  \n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        image_path = os.path.join(output_folder, f\"page_{i+1}.png\")\n",
    "        image.save(image_path, \"PNG\")\n",
    "\n",
    "        if is_blank(image):\n",
    "            break\n",
    "\n",
    "# Processes all PDFs in the input directory.\n",
    "# Extracts the year from the filename and creates a subfolder for each year.\n",
    "# Converts each PDF into images and stores them in the corresponding subfolder.\n",
    "def process_all_pdfs(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure the parent directory exists\n",
    "    \n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(input_dir, filename)  # Full path to PDF\n",
    "            year, start_page, end_page = extract_year_from_filename(filename)\n",
    "            if not year or year == \"Erreur\":  # Skip if invalid year extracted\n",
    "                continue\n",
    "\n",
    "            output_folder = os.path.join(output_dir, year)\n",
    "            os.makedirs(output_folder, exist_ok=True)  # Ensure the subfolder is created\n",
    "            \n",
    "            convert_pdf_to_images(pdf_path, output_folder,start_page,end_page)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "\n",
    "# Define the input and output paths relative to the current directory\n",
    "input_dir = os.path.join(current_dir, '..', 'data', 'pdf_Tables_theses_Paris_1870_1939')  # Relative path to 'lib/data/pdf_Tables_theses_Paris_1870_1939'\n",
    "output_dir = os.path.join(current_dir, '..', 'data', 'pdfs_en_images_png')  # Output path within 'lib/data/pdfs_en_images_png'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "process_all_pdfs(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIN DE L'EXTRACTION DE TEXTE;\n",
    "DEBUT DE L'EXTRACTIION ET DU TRAITEMENT DU TEXTE.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_all_tome_positions(image):\n",
    "    data = pytesseract.image_to_data(image, lang='fra', output_type=pytesseract.Output.DICT)\n",
    "    tome_positions = []\n",
    "\n",
    "    for i, text in enumerate(data[\"text\"]):\n",
    "        clean = text.strip().upper()\n",
    "        if not clean:\n",
    "            continue\n",
    "        if clean == \"TOME\" or re.search(r'\\bTOME\\s+\\w+\\b', clean):\n",
    "            y = data[\"top\"][i]\n",
    "            h = data[\"height\"][i]\n",
    "            tome_positions.append((y, h))\n",
    "\n",
    "    return sorted(tome_positions, key=lambda x: x[0])  # tri par ordre vertical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_tome_markers(image, positions):\n",
    "    slices = []\n",
    "    prev_y = 0\n",
    "\n",
    "    for y, h in positions:\n",
    "        cut_y = y + h\n",
    "        slices.append(image[prev_y:cut_y, :])\n",
    "        prev_y = cut_y\n",
    "\n",
    "    # Dernier segment (jusqu'en bas)\n",
    "    slices.append(image[prev_y:, :])\n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_columns(image):\n",
    "    (h, w) = image.shape\n",
    "    left = image[:, :w//2 - 5]\n",
    "    right = image[:, 5 + w//2:]    \n",
    "\n",
    "    left_text = pytesseract.image_to_string(left, lang='fra')\n",
    "    right_text = pytesseract.image_to_string(right, lang='fra')\n",
    "    return left_text + \" \" + right_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image, first):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    gray = clahe.apply(gray)\n",
    "\n",
    "    if first:\n",
    "        gray = gray[580:, :]\n",
    "\n",
    "    tome_positions = detect_all_tome_positions(gray)\n",
    "\n",
    "    if tome_positions:\n",
    "        horizontal_slices = split_by_tome_markers(gray, tome_positions)\n",
    "        full_text = \"\"\n",
    "\n",
    "        for i, slice_img in enumerate(horizontal_slices):\n",
    "            text = extract_columns(slice_img)\n",
    "            full_text += f\"--- Segment {i+1} ---\\n{text}\\n\\n\"\n",
    "\n",
    "        return full_text\n",
    "    else:\n",
    "        return extract_columns(gray)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_informations(text):\n",
    "\n",
    "    pattern = re.compile(r\"([A-ZÉÀÈÙÂÊÎÔÛÄËÏÖÜÇ]{2,}) \\(([^)]+)\\)\\. (.+)?\")\n",
    "\n",
    "    # Initialisation des variables\n",
    "    data = []\n",
    "    current_nom = None\n",
    "    current_prenom = None\n",
    "    current_sujet = \"\"                      \n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()  \n",
    "        \n",
    "        match = pattern.match(line)  \n",
    "\n",
    "        if match:\n",
    "            if current_nom is not None:\n",
    "                data.append([current_nom, current_prenom, current_sujet.strip()])\n",
    "\n",
    "            current_nom = match.group(1)\n",
    "            current_prenom = match.group(2)\n",
    "            current_sujet = match.group(3) if match.group(3) else \"\"\n",
    "        \n",
    "        else:\n",
    "            if current_nom is not None:\n",
    "                current_sujet += \" \" + line  \n",
    "\n",
    "    if current_nom is not None:\n",
    "        data.append([current_nom, current_prenom, current_sujet.strip()])\n",
    "\n",
    "    df_cleaned = pd.DataFrame(data, columns=[\"Nom\", \"Prénom\", \"Sujet\"])\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(input_dir):\n",
    "    text = \"\"  \n",
    "    df = pd.DataFrame()\n",
    "  \n",
    "    \n",
    "    for image_file in natsort.natsorted(os.listdir(input_dir)):\n",
    "        image_path = os.path.join(input_dir, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        if image_file.lower().endswith('.png'): \n",
    "            if  image_file == \"page_1.png\" : \n",
    "                first = True \n",
    "            else : first = False\n",
    "            text = text + process_image(image, first)\n",
    "    df = extract_informations(text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(text):\n",
    "    if isinstance(text, str):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', text)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "    return text\n",
    "\n",
    "def cleanup_up_subject_column(df) :\n",
    "    df['Nom'] = df['Nom'].astype(str)\n",
    "    df['Prénom'] = df['Prénom'].astype(str)\n",
    "    df['Sujet'] = df['Sujet'].astype(str)\n",
    "    df['Sujet'] = df['Sujet'].str.replace('- ', '', regex=False)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r',?\\s*\\d+\\s*', ' ', regex=True)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r'\\s+', ' ', regex=True).str.strip() \n",
    "    df['Sujet'] = df['Sujet'].str.replace(r\"([bcdfghjklmnpqrstvwxyz]) (?=[aeiouy])\", r\"\\1\", regex=True)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r\"[\\\"'#%&*\\[\\]{}<>|\\\\/^¤§°@=+\\~`]\", \"\", regex=True)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r\"[•●▪■♦◊¤§°@©®™‰¨«»„”“†‡‚‘’]\", \" \", regex=True)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r\"\\b[a-zA-Z]\\b\", \"\", regex=True)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r\"\\b(TOM|TON|H|U|UVIL|co|tu|ot|mw|vf|En|El|N)\\b\", \"\", regex=True, flags=re.IGNORECASE)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r\"\\s*[\\.,;:!?]\\s*\", \". \", regex=True)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    df['Sujet'] = df['Sujet'].apply(remove_accents)\n",
    "    df['Prénom'] = df['Prénom'].apply(remove_accents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescue_missing_entries(df):\n",
    "    \"\"\"\n",
    "    Looks for names accidentally merged into the 'Sujet' field,\n",
    "    and splits them out into new rows.\n",
    "    \"\"\"\n",
    "    rescue_pattern = re.compile(\n",
    "        r\"\\b([A-ZÉÀÈÙÂÊÎÔÛÄËÏÖÜÇ]{2,})[ ,]*([A-ZÉÀÈÙÂÊÎÔÛÄËÏÖÜÇa-zéàèùâêîôûäëïöüç\\-]+)\\)?[ .,-]\"\n",
    "    )\n",
    "\n",
    "    rescued_rows = []\n",
    "\n",
    "    for idx, sujet in df[\"Sujet\"].items():\n",
    "        matches = list(rescue_pattern.finditer(sujet))\n",
    "\n",
    "        if matches:\n",
    "            match = matches[0]\n",
    "            start = match.start()\n",
    "            rescued_text = sujet[start:]\n",
    "            original_subject = sujet[:start].strip()\n",
    "\n",
    "            # Update current row's Sujet\n",
    "            df.at[idx, \"Sujet\"] = original_subject\n",
    "\n",
    "            # Create rescued row\n",
    "            nom = match.group(1)\n",
    "            prenom = match.group(2)\n",
    "            sujet_rescue = rescued_text[len(match.group(0)):].strip()\n",
    "\n",
    "            rescued_rows.append({\n",
    "                \"Nom\": nom,\n",
    "                \"Prénom\": prenom,\n",
    "                \"Sujet\": sujet_rescue\n",
    "            })\n",
    "\n",
    "    if rescued_rows:\n",
    "        df_rescue = pd.DataFrame(rescued_rows)\n",
    "        df = pd.concat([df, df_rescue], ignore_index=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_folders(input_dir) : \n",
    "    df= pd.DataFrame(columns=[\"Nom\", \"Prénom\", \"Sujet\"])\n",
    "\n",
    "    for year_folder in natsort.natsorted(os.listdir(input_dir)):\n",
    "        year_path = os.path.join(input_dir, year_folder) \n",
    "        \n",
    "        if os.path.isdir(year_path):  # Check if it's a directory\n",
    "            df = pd.concat((df, process_folder(year_path)), ignore_index= True)\n",
    "            rescue_missing_entries(df)\n",
    "            cleanup_up_subject_column(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "input_dir = os.path.join(current_dir, '..', 'data', 'test')  # Relative path to 'lib/data/pdfs_en_images_png'\n",
    "\n",
    "df = process_all_folders(input_dir)\n",
    "print(df.to_string(max_rows=250, max_cols=210))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
