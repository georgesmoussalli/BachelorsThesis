{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import cv2\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd\n",
    "import natsort\n",
    "import matplotlib.pyplot as plt\n",
    "import spellchecker\n",
    "import rapidfuzz\n",
    "import unidecode \n",
    "import tqdm \n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = '/opt/homebrew/bin/tesseract'\n",
    "config = '--oem 2'\n",
    "spell = spellchecker.SpellChecker(language='fr') # SpellChecker en français\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)      # Show all rows\n",
    "pd.set_option(\"display.max_columns\", None)   # Show all columns\n",
    "pd.set_option(\"display.width\", 0)            # Auto-detect width\n",
    "pd.set_option(\"display.max_colwidth\", None)  # Don't truncate column content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the year from the filename after 'Thèses_'.\n",
    "def extract_year_from_filename(filename):\n",
    "    # Regex pattern to match both formats\n",
    "    match = re.search(r'Theses_(\\d{4})(?:_(\\d{4}))?_(\\d+)_(\\d+|blank)(?:\\.pdf)?$', filename)\n",
    "    \n",
    "    # If no match is found, return None or handle as needed\n",
    "    if not match:\n",
    "        return None\n",
    "\n",
    "    # Extract the year and check if there's a second year\n",
    "    year = match.group(1)\n",
    "    second_year = match.group(2)  # This will be None if there's no second year\n",
    "    start = match.group(3)\n",
    "    end = match.group(4)\n",
    "\n",
    "    # If a second year exists, concatenate it with the first year\n",
    "    if second_year:\n",
    "        year = f\"{year}_{second_year}\"\n",
    "\n",
    "    return (year, int(start), end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks if a page is blank \n",
    "def is_blank(image, threshold=0.99):\n",
    "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
    "    height = gray.shape[0]\n",
    "    \n",
    "    # Crops the 10% at the bottom of the page so that the number of the page doesn't affect our analysis\n",
    "    cropped_gray = gray[:int(0.9 * height), :]\n",
    "    \n",
    "    _, thresh = cv2.threshold(cropped_gray, 240, 255, cv2.THRESH_BINARY)\n",
    "    white_ratio = np.sum(thresh == 255) / thresh.size\n",
    "    \n",
    "    return white_ratio > threshold  # Retourne True si la page est blanche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a PDF file into images, saving each page as a PNG.\n",
    "# # Images are stored in the specified output folder.\n",
    "def convert_pdf_to_images(pdf_path, output_folder, start_page, end_page):\n",
    "    images = convert_from_path(pdf_path)\n",
    "    if end_page =='blank' :\n",
    "        images = images[start_page:] \n",
    "    else : \n",
    "        end_page = int(end_page)\n",
    "        images = images[start_page:end_page+1]  \n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        image_path = os.path.join(output_folder, f\"page_{i+1}.png\")\n",
    "        image.save(image_path, \"PNG\")\n",
    "\n",
    "        if is_blank(image):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processes all PDFs in the input directory.\n",
    "# Extracts the year from the filename and creates a subfolder for each year.\n",
    "# Converts each PDF into images and stores them in the corresponding subfolder.\n",
    "def process_all_pdfs(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure the parent directory exists\n",
    "    \n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(input_dir, filename)  # Full path to PDF\n",
    "            year, start_page, end_page = extract_year_from_filename(filename)\n",
    "            if not year or year == \"Erreur\":  # Skip if invalid year extracted\n",
    "                continue\n",
    "\n",
    "            output_folder = os.path.join(output_dir, year)\n",
    "            os.makedirs(output_folder, exist_ok=True)  # Ensure the subfolder is created\n",
    "            \n",
    "            convert_pdf_to_images(pdf_path, output_folder,start_page,end_page)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and output paths relative to the current directory\n",
    "input_dir = os.path.join(current_dir, '..', 'data', 'pdf_Tables_theses_Paris_1870_1939')  # Relative path to 'lib/data/pdf_Tables_theses_Paris_1870_1939'\n",
    "output_dir = os.path.join(current_dir, '..', 'data', 'pdfs_en_images_png')  # Output path within 'lib/data/pdfs_en_images_png'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "process_all_pdfs(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIN DE LA TRANSFORMATION EN PNG DES FICHIERS;\n",
    "DEBUT DE L'EXTRACTIION ET DU TRAITEMENT DU TEXTE.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_all_tome_positions(image):\n",
    "    data = pytesseract.image_to_data(image, lang='fra', output_type=pytesseract.Output.DICT)\n",
    "    tome_positions = []\n",
    "\n",
    "    for i, text in enumerate(data[\"text\"]):\n",
    "        clean = text.strip().upper()\n",
    "        if not clean:\n",
    "            continue\n",
    "        if clean == \"TOME\" or re.search(r'\\bTOME\\s+\\w+\\b', clean):\n",
    "            y = data[\"top\"][i]\n",
    "            h = data[\"height\"][i]\n",
    "            tome_positions.append((y, h))\n",
    "\n",
    "    return sorted(tome_positions, key=lambda x: x[0])  # tri par ordre vertical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_tome_markers(image, positions):\n",
    "    slices = []\n",
    "    prev_y = 0\n",
    "\n",
    "    for y, h in positions:\n",
    "        cut_y = y + h\n",
    "        slices.append(image[prev_y:cut_y, :])\n",
    "        prev_y = cut_y\n",
    "\n",
    "    # Dernier segment (jusqu'en bas)\n",
    "    slices.append(image[prev_y:, :])\n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_columns(image):\n",
    "    (h, w) = image.shape\n",
    "    left = image[:, :w//2 - 5]\n",
    "    right = image[:, 5 + w//2:]    \n",
    "\n",
    "    left_text = pytesseract.image_to_string(left, lang='fra', config=config)\n",
    "    right_text = pytesseract.image_to_string(right, lang='fra', config=config)\n",
    "    return re.sub(r'\\d+', '', left_text + \" \" + right_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image, first):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    scale_percent = 250 \n",
    "    width = int(gray.shape[1] * scale_percent / 100)\n",
    "    height = int(gray.shape[0] * scale_percent / 100)      \n",
    "    dim = (width, height)\n",
    "    gray = cv2.resize(gray, dim, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    gray = clahe.apply(gray)\n",
    "\n",
    "    gray = cv2.medianBlur(gray, 3)\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,2))\n",
    "    gray = cv2.dilate(gray, kernel, iterations=1)\n",
    "\n",
    "    if first:\n",
    "        gray = gray[580:, :]\n",
    "\n",
    "    tome_positions = detect_all_tome_positions(gray)\n",
    "\n",
    "    if tome_positions:\n",
    "        horizontal_slices = split_by_tome_markers(gray, tome_positions)\n",
    "        full_text = \"\"\n",
    "\n",
    "        for i, slice_img in enumerate(horizontal_slices):\n",
    "            text = extract_columns(slice_img)\n",
    "            full_text += f\"--- Segment {i+1} ---\\n{text}\\n\\n\"\n",
    "\n",
    "        return full_text\n",
    "    else:\n",
    "        return extract_columns(gray)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_informations(text, year):\n",
    "\n",
    "    pattern = re.compile(r\"([A-ZÉÀÈÙÂÊÎÔÛÄËÏÖÜÇ]{2,})(?: \\(([^)]+)\\))?\\. (.+)?\")\n",
    "\n",
    "    # Initialisation des variables\n",
    "    data = []\n",
    "    current_nom = None\n",
    "    current_prenom = None\n",
    "    current_sujet = \"\"        \n",
    "    current_year = year             \n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()  \n",
    "        \n",
    "        match = pattern.match(line)  \n",
    "\n",
    "        if match:\n",
    "            if current_nom is not None:\n",
    "                data.append([current_nom, current_prenom, current_sujet.strip(),current_year])\n",
    "\n",
    "            current_nom = match.group(1)\n",
    "            current_prenom = match.group(2)\n",
    "            current_sujet = match.group(3) if match.group(3) else \"\"\n",
    "        \n",
    "        else:\n",
    "            if current_nom is not None:\n",
    "                current_sujet += \" \" + line  \n",
    "\n",
    "    if current_nom is not None:\n",
    "        data.append([current_nom, current_prenom, current_sujet.strip(), current_year])\n",
    "\n",
    "    df_cleaned = pd.DataFrame(data, columns=[\"Nom\", \"Prénom\", \"Sujet\",\"year\"])\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(input_dir,year):\n",
    "    text = \"\"  \n",
    "    df = pd.DataFrame()\n",
    "  \n",
    "    \n",
    "    for image_file in natsort.natsorted(os.listdir(input_dir)):\n",
    "        image_path = os.path.join(input_dir, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        if image_file.lower().endswith('.png'): \n",
    "            if  image_file == \"page_1.png\" : \n",
    "                first = True \n",
    "            else : first = False\n",
    "            text = text + process_image(image, first)\n",
    "    print(text)\n",
    "    df = extract_informations(text,year)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(text):\n",
    "    if isinstance(text, str):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', text)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_up_subject_column(df) :\n",
    "    df['Nom'] = df['Nom'].astype(str)\n",
    "    df['Prénom'] = df['Prénom'].astype(str)\n",
    "    df['Sujet'] = df['Sujet'].astype(str)\n",
    "    df['Sujet'] = df['Sujet'].str.replace('- ', '', regex=False)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r',?\\s*\\d+\\s*', ' ', regex=True)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r'\\s+', ' ', regex=True).str.strip() \n",
    "    df['Sujet'] = df['Sujet'].str.replace(r\"([bcdfghjklmnpqrstvwxyz]) (?=[aeiouy])\", r\"\\1\", regex=True)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r\"[\\\"'#%&*\\[\\]{}<>|\\\\/^¤§°@=+\\~`]\", \"\", regex=True)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r\"[•●▪■♦◊¤§°@©®™‰¨«»„”“†‡‚‘’]\", \" \", regex=True)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r\"\\b[a-zA-Z]\\b\", \"\", regex=True)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r\"\\b(TOM|TON|H|U|UVIL|co|tu|ot|mw|vf|En|El|N)\\b\", \"\", regex=True, flags=re.IGNORECASE)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r\"\\s*[\\.,;:!?]\\s*\", \". \", regex=True)\n",
    "    df['Sujet'] = df['Sujet'].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    df['Sujet'] = df['Sujet'].apply(remove_accents)\n",
    "    df['Prénom'] = df['Prénom'].apply(remove_accents).str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_folders(input_dir) : \n",
    "    df= pd.DataFrame(columns=[\"Nom\", \"Prénom\", \"Sujet\", \"year\"])\n",
    "\n",
    "    for year_folder in natsort.natsorted(os.listdir(input_dir)):\n",
    "        year_path = os.path.join(input_dir, year_folder) \n",
    "        print(year_folder)        \n",
    "        if os.path.isdir(year_path):  # Check if it's a directory\n",
    "            df = pd.concat((df, process_folder(year_path, year_folder)), ignore_index= True)\n",
    "            cleanup_up_subject_column(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = os.path.join(current_dir, '..', 'data', 'test')  # Relative path to 'lib/data/pdfs_en_images_png'\n",
    "\n",
    "df1 = process_all_folders(input_dir)\n",
    "print(df1.to_string(max_rows=250, max_cols=210))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIN  DE L'EXTRACTION DE TEXTE ET CORRECTIONS MINEURES D'OCR. \n",
    "DEBUT DU TRAITEMENT DE TEXTE : \n",
    "PRECISION ET CORRECTION DE L'OCR (DICTIONNAIRE FRANCAIS ET MEDICAL)\n",
    "Corriger les mots faux de l ocrisation a l aide des dictionnaires: \n",
    "tu pourrais envisager une correction automatique OCR après reconnaissance (spellchecker français + médical).\n",
    "\n",
    "Exemples de librairies :\n",
    "\n",
    "pyspellchecker\n",
    "fuzzywuzzy pour match flous avec ton francais.txt\n",
    "\n",
    "Une fois que la correction est faire refaire test de precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = input_dir = os.path.join(current_dir, '..', 'data', 'Dictionnaires')  # Relative path to 'lib/data/pdfs_en_images_png'\n",
    "\n",
    "# Chargement + nettoyage du dictionnaire français\n",
    "with open(os.path.join(input_dir, 'francais.txt'), encoding=\"utf-8\") as f:\n",
    "    dico_fr = set(remove_accents(line.strip().lower()) for line in f if line.strip())\n",
    "\n",
    "with open(os.path.join(input_dir, 'medical.txt'), encoding=\"utf-8\") as f:\n",
    "    dico_med = set(remove_accents(line.strip().lower()) for line in f if line.strip())\n",
    "\n",
    "# Lecture manuelle pour ignorer les erreurs d'encodage\n",
    "with open(os.path.join(input_dir, 'prenoms.csv'), encoding='utf-8', errors='replace') as f:\n",
    "    prenoms_df = pd.read_csv(f, sep=';')\n",
    "\n",
    "# Nettoyage et transformation en set\n",
    "dico_prenoms = set(\n",
    "    remove_accents(prenom.strip().lower())\n",
    "    for prenom in prenoms_df['01_prenom']\n",
    "    if isinstance(prenom, str) and prenom.strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compter_mots(text, dico_fr, dico_med, dico_prenoms):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0, 0\n",
    "    mots = re.findall(r'\\b\\w+\\b', text.lower())  # Découpe en mots\n",
    "    total = len(mots)\n",
    "    corrects = sum(\n",
    "        1 for mot in mots if mot in dico_fr or (dico_med and mot in dico_med) or (dico_prenoms and mot in dico_prenoms)\n",
    ")\n",
    "    return corrects, total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculer_precision_OCR(df, dico_fr, dico_med, dico_prenoms):\n",
    "    df = df.copy() \n",
    "\n",
    "    df[['Mots corrects', 'Mots totaux']] = df['Sujet'].apply(lambda x: pd.Series(compter_mots(x, dico_fr, dico_med, dico_prenoms)))\n",
    "\n",
    "    df2 = df.groupby('year').agg({'Mots corrects': 'sum', 'Mots totaux': 'sum'}).reset_index()\n",
    "\n",
    "    total_row = pd.DataFrame({\n",
    "        'year': ['Total'],\n",
    "        'Mots corrects': [df2['Mots corrects'].sum()],\n",
    "        'Mots totaux': [df2['Mots totaux'].sum()]\n",
    "    })\n",
    "\n",
    "    df2 = pd.concat([df2, total_row], ignore_index=True)\n",
    "\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afficher_précision(df, dico_fr, dico_med, dico_prenoms) :\n",
    "    precision_df = calculer_precision_OCR(df, dico_fr, dico_med, dico_prenoms)\n",
    "    print(precision_df.to_string(max_rows=1000000, max_cols=5))\n",
    "    precision_df['Precision (%)'] = precision_df['Mots corrects'] / precision_df['Mots totaux'] * 100\n",
    "\n",
    "    total_correct = precision_df.loc[precision_df['year'] == 'Total', 'Mots corrects'].values[0]\n",
    "    total_words = precision_df.loc[precision_df['year'] == 'Total', 'Mots totaux'].values[0]\n",
    "    total_incorrect = total_words - total_correct\n",
    "\n",
    "    labels = ['Mots Corrects', 'Mots Incorrects']\n",
    "    sizes = [total_correct, total_incorrect]\n",
    "    colors = ['#4CAF50', '#F44336']\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
    "    plt.title('Répartition globale des mots (OCR)')\n",
    "    plt.axis('equal')  \n",
    "    plt.show()\n",
    "\n",
    "    precision_df_years_only = precision_df[precision_df['year'] != 'Total']\n",
    "\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.bar(precision_df_years_only['year'], precision_df_years_only['Precision (%)'], color='skyblue')\n",
    "    plt.title('Évolution de la précision OCR par année')\n",
    "    plt.xlabel('Année')\n",
    "    plt.ylabel('Précision (%)')\n",
    "    plt.grid(axis='y')\n",
    "    plt.xticks(rotation=90, fontsize=6)  # ← rotation 90° pour les années\n",
    "    plt.ylim(0, 100)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return precision_df\n",
    "\n",
    "df2 = afficher_précision(df1, dico_fr, dico_med, dico_prenoms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = unidecode.unidecode(text.lower()) \n",
    "    text = re.sub(r\"[^a-zàâçéèêëîïôûùüÿñæœ' -]\", ' ', text)  \n",
    "    return text.strip()\n",
    "\n",
    "def correct_token(token, vocab, threshold=85):\n",
    "    if token in vocab:\n",
    "        return token\n",
    "    match = rapidfuzz.process.extractOne(token, vocab, score_cutoff=threshold)\n",
    "    if match:\n",
    "        return match[0]  # match is (match_string, score, index)\n",
    "    return token  \n",
    "\n",
    "def correct_ocr_text(text, vocab):\n",
    "    text = preprocess(text)\n",
    "    tokens = text.split()\n",
    "    corrected_tokens = [correct_token(t, vocab) for t in tokens]\n",
    "    return ' '.join(corrected_tokens)\n",
    "\n",
    "tqdm.tqdm.pandas()  \n",
    "df3 = df1.copy()\n",
    "df3['Sujet'] = df1['Sujet'].progress_apply(lambda x: correct_ocr_text(x, set(dico_fr) | set(dico_med)))\n",
    "\n",
    "df3['Prénom'] = df1['Prénom'].progress_apply(lambda x: correct_ocr_text(x,set(dico_prenoms)))\n",
    "\n",
    "df4 = afficher_précision(df3, dico_fr, dico_med, dico_prenoms)\n",
    "\n",
    "print(df3.to_string(max_rows=250, max_cols=210))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RECONNAISSANCE DU SEXE (DICTIONNAIRE DES PRENOMS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = input_dir = os.path.join(current_dir, '..', 'data', 'Dictionnaires')  # Relative path to 'lib/data/pdfs_en_images_png'\n",
    "\n",
    "df_prenoms = pd.read_csv(os.path.join(input_dir, 'Prenoms.csv'), sep=\";\", header=None, names=[\"Prénom\", \"genre\", \"langue\", \"frequence\"], encoding='ISO-8859-1')\n",
    "\n",
    "dictionnaire_prenoms = dict(zip(df_prenoms['Prénom'], df_prenoms['genre']))\n",
    "\n",
    "def detect_genre(prenom):\n",
    "    if not isinstance(prenom, str):\n",
    "        return \"Inconnu\"\n",
    "\n",
    "    prenom_clean = remove_accents(prenom).lower()\n",
    "    composants = re.split(r'[-\\s]', prenom_clean)\n",
    "\n",
    "    genres_detectes = set()\n",
    "    for comp in composants:\n",
    "        genre = dictionnaire_prenoms.get(comp)\n",
    "        if genre:\n",
    "            genres_detectes.update(genre.split(','))\n",
    "\n",
    "    if not genres_detectes:\n",
    "        return 'Inconnu'\n",
    "    \n",
    "    # Logique de priorité\n",
    "    if genres_detectes == {'m'}:\n",
    "        return 'm'\n",
    "    elif genres_detectes == {'f'}:\n",
    "        return 'f'\n",
    "    elif 'm' in genres_detectes and 'f' not in genres_detectes:\n",
    "        return 'm'\n",
    "    elif 'f' in genres_detectes and 'm' not in genres_detectes:\n",
    "        return 'f'\n",
    "    elif 'm' in genres_detectes and 'f' in genres_detectes:\n",
    "        return 'm'  # ← on priorise \"m\" dans les cas mixtes\n",
    "    else:\n",
    "        return 'Inconnu'\n",
    "\n",
    "\n",
    "\n",
    "df3['Prénom'] = df3['Prénom'].apply(remove_accents).str.lower()\n",
    "\n",
    "df3['genre'] = df3['Prénom'].apply(detect_genre)\n",
    "\n",
    "print(df3.to_string(max_rows=250, max_cols=210))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculer_genres_par_annee(df):\n",
    "    # Compte des genres par année\n",
    "    df_genre = df.groupby(['year', 'genre']).size().unstack(fill_value=0).reset_index()\n",
    "\n",
    "    # Ajout de colonnes manquantes si nécessaire (ex: si 'm' ou 'f' absents dans certaines années)\n",
    "    for genre in ['f', 'm', 'Inconnu']:\n",
    "        if genre not in df_genre.columns:\n",
    "            df_genre[genre] = 0\n",
    "\n",
    "    # Calcul total tous genres\n",
    "    total_row = pd.DataFrame({\n",
    "        'year': ['Total'],\n",
    "        'f': [df_genre['f'].sum()],\n",
    "        'm': [df_genre['m'].sum()],\n",
    "        'Inconnu': [df_genre['Inconnu'].sum()]\n",
    "    })\n",
    "\n",
    "    # Fusion avec les lignes par année\n",
    "    df_genre = pd.concat([df_genre, total_row], ignore_index=True)\n",
    "\n",
    "    return df_genre\n",
    "\n",
    "def afficher_genres_par_annee(df):\n",
    "    genre_df = calculer_genres_par_annee(df)\n",
    "\n",
    "    # Affichage tableau\n",
    "    print(genre_df.to_string(index=False))\n",
    "\n",
    "    # Affichage du camembert global\n",
    "    total = genre_df[genre_df['year'] == 'Total']\n",
    "    sizes = [total['f'].values[0], total['m'].values[0], total['Inconnu'].values[0]]\n",
    "    labels = ['Femmes', 'Hommes', 'Inconnu']\n",
    "    colors = ['#FF69B4', '#87CEFA', '#D3D3D3']\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
    "    plt.title(\"Répartition globale par sexe\")\n",
    "    plt.axis(\"equal\")\n",
    "    plt.show()\n",
    "\n",
    "    # Affichage par année\n",
    "    genre_df_years_only = genre_df[genre_df['year'] != 'Total']\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.plot(genre_df_years_only['year'], genre_df_years_only['f'], label='Femmes', color='#FF69B4')\n",
    "    plt.plot(genre_df_years_only['year'], genre_df_years_only['m'], label='Hommes', color='#87CEFA')\n",
    "    plt.plot(genre_df_years_only['year'], genre_df_years_only['Inconnu'], label='Inconnu', color='#A9A9A9')\n",
    "    plt.legend()\n",
    "    plt.title(\"Répartition des sexes par année\")\n",
    "    plt.xlabel(\"Année\")\n",
    "    plt.ylabel(\"Nombre d'individus\")\n",
    "    plt.xticks(rotation=90, fontsize=6)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return genre_df\n",
    "\n",
    "df_genre = afficher_genres_par_annee(df3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_filtré = df3[df3['genre'] != 'Inconnu'].copy()\n",
    "\n",
    "df_genre = afficher_genres_par_annee(df3_filtré)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANALYSE THEMATIQUE "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
